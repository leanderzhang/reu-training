{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPOqghHjhxJxT4Ye8nWWCyL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Xn3E4mjPCtY","executionInfo":{"status":"ok","timestamp":1691004172486,"user_tz":240,"elapsed":11427,"user":{"displayName":"Stephen Qiu","userId":"08150546758401848037"}},"outputId":"afbc66e0-31ff-47e1-8204-f167ae776b56"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total loss = tf.Tensor([1.2213281], shape=(1,), dtype=float32)\n","The new w0 is [[[[ 0.7730709]]\n","\n","  [[-0.0269291]]]\n","\n","\n"," [[[-0.0269291]]\n","\n","  [[ 0.7730709]]]]\n","The new w1 is [[-0.03398073  0.36601928]\n"," [ 0.17884515  0.57884514]\n"," [ 0.27884516  0.67884517]\n"," [ 0.26601928  0.6660193 ]]\n","The new w2 is [[ 0.55835515 -0.15835512]\n"," [ 1.2788975  -0.6788975 ]]\n","The new b0 is -0.15385821\n","The new b1 is 0.058967665\n","The new b2 is 0.3\n"]}],"source":["import tensorflow as tf\n","import keras\n","\n","input = tf.constant([[1,0,1],[0,1,0],[1,0,1]],shape=[1,3,3,1],dtype=tf.float32)\n","filter = tf.constant([[0.9,0.1],[0.1,0.9]],shape=[2,2,1,1],dtype=tf.float32)\n","b0 = tf.constant(0.1)\n","conv_output = tf.nn.conv2d(input,filter,strides=1,padding='VALID')+b0\n","w0 = filter\n","\n","relu_output = tf.nn.relu(conv_output)\n","flat = tf.reshape(relu_output,[1,4])\n","x = flat\n","w1 = tf.constant([[0.1,0.5],[0.2,0.6],[0.3,0.7],[0.4,0.8]])\n","w2 = tf.constant([[0.1,0.3],[0.2,0.4]])\n","t = tf.constant([[1,0]])\n","b1 = tf.constant(0.2)\n","b2 = tf.constant(0.3)\n","\n","def forward(x,w1,w2,b1,b2):\n","    sum1=tf.matmul(x,w1)+b1\n","    H = sum1\n","    sum2=tf.matmul(H,w2)+b2\n","    O = tf.nn.softmax(sum2)\n","    return O\n","\n","def backward(x,w1,w2,b1,b2,t):\n","    with tf.GradientTape() as g:\n","       g.watch([w1,w2,x,b1,b2])\n","       O = forward(x, w1, w2, b1, b2)\n","       Etotal = -tf.math.log(O[:, 0])\n","       print(\"Total loss = \" + str(Etotal))\n","    dEtotal_dw = g.gradient(Etotal, [w1,w2,x,b1,b2])\n","    dEtotal_dw1, dEtotal_dw2, dEtotal_dx, dEtotal_db1, dEtotal_db2 = dEtotal_dw\n","    return dEtotal_dw1,dEtotal_dw2,dEtotal_dx,dEtotal_db1,dEtotal_db2\n","\n","dEtotal_dw1,dEtotal_dw2,dEtotal_dx,dEtotal_db1,dEtotal_db2 = backward(x,w1,w2,b1,b2,t)\n","\n","w1 = w1 - 0.5*dEtotal_dw1 #the learning rate is set to be 0.5\n","w2 = w2 - 0.5*dEtotal_dw2\n","\n","b1 = b1 - 0.5*dEtotal_db1\n","b2 = b2 - 0.5*dEtotal_db2\n","\n","dfilter = tf.constant(dEtotal_dx.numpy(),shape=[2,2,1,1],dtype=tf.float32)\n","dEtotal_dw0 = tf.nn.conv2d(input,dfilter,strides = 1,padding ='VALID')\n","dEtotal_dw0 = tf.reshape(dEtotal_dw0,[2,2,1,1])\n","w0 = w0 - 0.5*dEtotal_dw0\n","\n","dEtotal_db0 = tf.reduce_sum(dEtotal_dx)\n","b0 = b0 - 0.5*dEtotal_db0\n","print('The new w0 is',w0.numpy())\n","print('The new w1 is',w1.numpy())\n","print('The new w2 is',w2.numpy())\n","print('The new b0 is',b0.numpy())\n","print('The new b1 is',b1.numpy())\n","print('The new b2 is',b2.numpy())"]}]}